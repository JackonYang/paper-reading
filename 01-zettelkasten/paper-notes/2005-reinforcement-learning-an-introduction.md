---
title: Reinforcement Learning - An Introduction
authors:
- R. Sutton
- A. Barto
fieldsOfStudy:
- Computer Science
meta_key: 2005-reinforcement-learning-an-introduction
numCitedBy: 32987
reading_status: TBD
ref_count: 636
tags:
- gen-from-ref
- paper
venue: IEEE Transactions on Neural Networks
year: 2005
---

# Reinforcement Learning - An Introduction

## Abstract

Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.

## Paper References

1. Reinforcement Learning - A Survey
2. Self-improving reactive agents based on reinforcement learning, planning and teaching
3. Problem solving with reinforcement learning
4. Input Generalization in Delayed Reinforcement Learning - An Algorithm and Performance Comparisons
5. On the Computational Economics of Reinforcement Learning
6. Importance sampling for reinforcement learning with multiple objectives
7. Adaptive Confidence and Adaptive Curiosity
8. Modular on-line function approximation for scaling up reinforcement learning
9. Gradient Descent for General Reinforcement Learning
10. Adaptive Critics and the Basal Ganglia
11. Reinforcement Learning with a Hierarchy of Abstract Models
12. Learning and problem-solving with multilayer connectionist systems (adaptive, strategy learning, neural networks, reinforcement learning)
13. Large-scale dynamic optimization using teams of reinforcement learning agents
14. Recent Advances in Hierarchical Reinforcement Learning
15. Scaling Reinforcement Learning Algorithms by Learning Variable Temporal Resolution Models
16. Handbook of Learning and Approximate Dynamic Programming
17. Learning in embedded systems
18. Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems
19. On the sample complexity of reinforcement learning.
20. Neuronlike adaptive elements that can solve difficult learning control problems
21. Continual learning in reinforcement environments
22. Incorporating Advice into Agents that Learn from Reinforcements
23. Consistency of HDP applied to a simple reinforcement learning problem
24. Learning to Solve Markovian Decision Processes
25. [Simple statistical gradient-following algorithms for connectionist reinforcement learning](2004-simple-statistical-gradient-following-algorithms-for-connectionist-reinforcement-learning)
26. Reinforcement-learning control and pattern recognition systems
27. Explanation-Based Learning and Reinforcement Learning - A Unified View
28. Reinforcement Learning Applied to Linear Quadratic Regulation
29. On the use of backpropagation in associative reinforcement learning
30. Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming
31. On-line Q-learning using connectionist systems
32. Average reward reinforcement learning - Foundations, algorithms, and empirical results
33. Temporal abstraction in reinforcement learning
34. Shaping and policy search in reinforcement learning
35. Reinforcement Learning With High-Dimensional, Continuous Actions
36. Model-Based Reinforcement Learning with an Approximate, Learned Model
37. Incremental multi-step Q-learning
38. Training Agents to Perform Sequential Behavior
39. R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning
40. Monte Carlo Matrix Inversion and Reinforcement Learning
41. Truncating Temporal Differences - On the Efficient Implementation of TD(lambda) for Reinforcement Learning
42. On integrating apprentice learning and reinforcement learning TITLE2
43. H-Learning - A Reinforcement Learning Method for Optimizing Undiscounted Average Reward
44. Learning to Act Using Real-Time Dynamic Programming
45. [Artificial Intelligence - A Modern Approach](1995-artificial-intelligence-a-modern-approach)
46. Truncating Temporal Diierences - on the Eecient Implementation of Td() for Reinforcement Learning
47. Policy Gradient Methods for Reinforcement Learning with Function Approximation
48. Infinite-Horizon Policy-Gradient Estimation
49. Generalization in Reinforcement Learning - Successful Examples Using Sparse Coarse Coding
50. Reward Functions for Accelerated Learning
51. Models of Learning Systems.
52. Markov Games as a Framework for Multi-Agent Reinforcement Learning
53. Potential-Based Shaping and Q-Value Initialization are Equivalent
54. Shaping as a method for accelerating reinforcement learning
55. Q-learning
56. Efficient memory-based learning for robot control
57. Learning Without State-Estimation in Partially Observable Markovian Decision Processes
58. Reinforcement Learning with Replacing Eligibility Traces
59. A MATHEMATICAL ANALYSIS OF ACTOR-CRITIC ARCHITECTURES FOR LEARNING OPTIMAL CONTROLS THROUGH INCREMENTAL DYNAMIC PROGRAMMING
60. Planning by Incremental Dynamic Programming
61. A Reinforcement Learning Method for Maximizing Undiscounted Rewards
62. Varieties of learning automata - an overview
63. Apprenticeship learning via inverse reinforcement learning
64. Reinforcement Learning with Soft State Aggregation
65. Learning and Sequential Decision Making
66. Goal Seeking Components for Adaptive Intelligence - An Initial Assessment.
67. On-line Policy Improvement using Monte-Carlo Search
68. Online Learning with Random Representations
69. A new approach to the design of reinforcement schemes for learning automata
70. An RLS-Based Natural Actor-Critic Algorithm for Locomotion of a Two-Linked Robot Arm
71. Learning Policies for Partially Observable Environments - Scaling Up
72. Learning to perceive and act by trial and error
73. Between MDPs and Semi-MDPs - A Framework for Temporal Abstraction in Reinforcement Learning
74. Toward a modern theory of adaptive networks - expectation and prediction.
75. Connectionist learning for control - an overview
76. A Teaching Method for Reinforcement Learning
77. Training and Tracking in Robotics
78. Reinforcement Learning, Spike-Time-Dependent Plasticity, and the BCM Rule
79. Scaling Up Average Reward Reinforcement Learning by Approximating the Domain Models and the Value Function
80. Pattern-recognizing stochastic learning automata
81. Experiments with Infinite-Horizon, Policy-Gradient Estimation
82. Least-Squares Policy Iteration
83. Learning of sequential movements by neural network model with dopamine-like reinforcement signal
84. A neural network model with dopamine-like reinforcement signal that learns a spatial delayed response task
85. A heuristic approach to reinforcement learning control systems
86. Residual Algorithms - Reinforcement Learning with Function Approximation
87. Comparing Policy-Gradient Algorithms
88. Reinforcement Learning Through Gradient Descent
89. Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms
90. Steps toward Artificial Intelligence
91. Punish/Reward - Learning with a Critic in Adaptive Threshold Systems
92. Reinforcement Learning with Perceptual Aliasing - The Perceptual Distinctions Approach
93. Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning
94. Feudal Reinforcement Learning
95. Learning by statistical cooperation of self-interested neuron-like computing elements.
96. Eligibility Traces for Off-Policy Policy Evaluation
97. Dyna, an integrated architecture for learning, planning, and reacting
98. Reinforcement Learning Methods for Continuous-Time Markov Decision Problems
99. A possibility for implementing curiosity and boredom in model-building neural controllers
100. Off-Policy Temporal Difference Learning with Function Approximation
101. A Predictive Reinforcement Model of Dopamine Neurons for Learning Approach Behavior
102. Incremental dynamic programming for on-line adaptive optimal control
103. On the Convergence of Stochastic Iterative Dynamic Programming Algorithms
104. Game-theoretic cooperativity in networks of self-interested units
105. Stable Function Approximation in Dynamic Programming
106. The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces
107. Local and Global Optimization Algorithms for Generalized Learning Automata
108. Near-Optimal Reinforcement Learning in Polynomial Time
109. Learning Automata - A Survey
110. Reinforcement Learning by Construction of Hypothetical Targets
111. Policy Invariance Under Reward Transformations - Theory and Application to Reward Shaping
112. A Reinforcement Learning Approach to job-shop Scheduling
113. Programming backgammon using self-teaching neural nets
114. Learning in neural networks by reinforcement of irregular spiking.
115. Timing and Partial Observability in the Dopamine System
116. Generalization in Reinforcement Learning - Safely Approximating the Value Function
117. Catastrophic Interference in Connectionist Networks - The Sequential Learning Problem
118. TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play
119. An Adaptive Optimal Controller for Discrete-Time Markov Environments
120. Finite-time Analysis of the Multiarmed Bandit Problem
121. Actor-critic models of the basal ganglia - new anatomical and computational perspectives
122. Generalization of backpropagation with application to a recurrent gas market model
123. Alopex - A Correlation-Based Learning Algorithm for Feedforward and Recurrent Neural Networks
124. Improving Elevator Performance Using Reinforcement Learning
125. Approximate solutions to markov decision processes
126. Automatic Programming of Behavior-Based Robots Using Reinforcement Learning
127. Sparse Distributed Memory
128. Intelligent Behavior as an Adaptation to the Task Environment
129. TD Models - Modeling the World at a Mixture of Time Scales
130. Hierarchical control and learning for markov decision processes
131. TD(位) Converges with Probability 1
132. Variations on the Boltzmann Machine Learning Algorithm
133. Reinforcement learning with modulated spike timing dependent synaptic plasticity.
134. Least-Squares Temporal Difference Learning
135. Decentralized learning in finite Markov chains
136. Real-Time Heuristic Search
137. REINFORCEMENT DRIVEN INFORMATION ACQUISITION IN NONDETERMINISTIC ENVIRONMENTS
138. Dynamic signals related to choices and outcomes in the dorsolateral prefrontal cortex.
139. Kernel-Based Reinforcement Learning
140. Reinforcement Learning with Function Approximation Converges to a Region
141. Value-dependent selection in the brain - Simulation in a synthetic neural model
142. Strategy Learning with Multilayer Connectionist Representations
143. Representation and Timing in Theories of the Dopamine System
144. Actor-Critic Algorithms
145. PVLV - the primary value and learned value Pavlovian learning algorithm.
146. Intrinsically Motivated Learning of Hierarchical Collections of Skills
147. A neuronal model of classical conditioning
148. Decomposition Techniques for Planning in Stochastic Domains
149. Reinforcement learning for job shop scheduling
150. Learning Algorithms for Two-Person Zero-Sum Stochastic Games with Incomplete Information - A Unified Approach
151. A Theory of Attention - Variations in the Associability of Stimuli with Reinforcement
152. Exploiting Structure in Policy Construction
153. STELLA - A scheme for a learning machine
154. PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS
155. Off-policy Learning with Options and Recognizers
156. Synthesis of nonlinear control surfaces by a layered associative search network
157. Technical Update - Least-Squares Temporal Difference Learning
158. Modeling functions of striatal dopamine modulation in learning and planning
159. Intrinsic Motivation Systems for Autonomous Mental Development
160. Prioritized Sweeping - Reinforcement Learning with Less Data and Less Real Time
161. Approximations of Dynamic Programs, I
162. Learning internal representations
163. Catastrophic forgetting in connectionist networks
164. Modified Policy Iteration Algorithms for Discounted Markov Decision Problems
165. Practical issues in temporal difference learning
166. Dynamic Programming
167. Overcoming Incomplete Perception with Utile Distinction Memory
168. Connectionist models of recognition memory - constraints imposed by learning and forgetting functions.
169. Reinforcement Learning Through Modulation of Spike-Timing-Dependent Synaptic Plasticity
170. Neural network learning and expert systems
171. The misbehavior of value and the discipline of the will
172. CMAC-based adaptive critic self-learning control
173. Associative search network - A reinforcement learning associative memory
174. Distinctive features, categorical perception, and probability learning - some applications of a neural model
175. Adapting Bias by Gradient Descent - An Incremental Version of Delta-Bar-Delta
176. Bee foraging in uncertain environments using predictive hebbian learning
177. Toward a Statistical Theory of Learning.
178. On The Virtues of Linear Learning and Trajectory Distributions
179. Simple neural models of classical conditioning
180. Learning control systems--Review and outlook
181. Local Gain Adaptation in Stochastic Gradient Descent
182. Learning the temporal dynamics of behavior.
183. Solving the distal reward problem through linkage of STDP and dopamine signaling
184. Curious model-building control systems
185. A neural model of adaptive behavior
186. Learning to control an inverted pendulum using neural networks
187. A model for Pavlovian learning - variations in the effectiveness of conditioned but not of unconditioned stimuli.
188. Learning to Predict by the Methods of Temporal Differences
189. [A Fast Learning Algorithm for Deep Belief Nets](2006-a-fast-learning-algorithm-for-deep-belief-nets)
190. [Learning Deep Architectures for AI](2007-learning-deep-architectures-for-ai)
191. [Neural Networks for Pattern Recognition](1993-neural-networks-for-pattern-recognition)
192. Efficient Memory-Based Dynamic Programming
193. Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control
194. A dynamic channel assignment policy through Q-learning
195. Dopamine neurons report an error in the temporal prediction of reward during learning
196. Bayesian theories of conditioning in a changing world
197. Optimal control-1950 to 1985
198. Theoretical Neuroscience - Computational and Mathematical Modeling of Neural Systems
199. Networks of Learning Automata - Techniques for Online Stochastic Optimization
200. A survey of algorithmic methods for partially observed Markov decision processes
201. A Machine with Insight
202. A computational model of action selection in the basal ganglia. II. Analysis and simulation of behaviour
203. Increased rates of convergence through learning rate adaptation
204. Temporal difference learning and TD-Gammon
205. Analysis of Temporal-Diffference Learning with Function Approximation
206. Timing in simple conditioning and occasion setting - a neural network approach
207. The predictive brain - temporal coincidence and temporal order in synaptic learning mechanisms.
208. Discrete Coding of Reward Probability and Uncertainty by Dopamine Neurons
209. A Natural Policy Gradient
210. Robot juggling - implementation of memory-based learning
211. Numerical dynamic programming in economics
212. [Long Short-Term Memory](1997-long-short-term-memory)
213. Predictive Representations of State
214. A learning machine with monologue
215. [Adaptation in natural and artificial systems](1975-adaptation-in-natural-and-artificial-systems)
216. Intrinsically Motivated Reinforcement Learning
217. Stochastic learning and optimization - A sensitivity-based approach
218. On the law of effect.
219. Making Working Memory Work - A Computational Model of Learning in the Prefrontal Cortex and Basal Ganglia
220. Human operators and automatic adaptive controllers - A comparative study on a particular control task
221. Responses of monkey dopamine neurons to reward and conditioned stimuli during successive steps of learning a delayed response task
222. The apparent conflict between estimation and control-a survey of the two-armed bandit problem
223. Random walks and electric networks
224. Using Aperiodic Reinforcement for Directed Self-Organization During Development
225. What is Intrinsic Motivation? A Typology of Computational Approaches
226. Actions and habits - the development of behavioural autonomy
227. Dopamine Cells Respond to Predicted Events during Classical Conditioning - Evidence for Eligibility Traces in the Reward-Learning Network
228. Numerical Methods for Stochastic Control Problems in Continuous Time
229. A theory of Pavlovian conditioning - Variations in the effectiveness of reinforcement and nonreinforcement
230. Temporal Difference Learning in Continuous Time and Space
231. A computational substrate for incentive salience
232. Tonic dopamine - opportunity costs and the control of response vigor
233. Q-Learning for Bandit Problems
234. Variations in the Sensitivity of Instrumental Responding to Reinforcer Devaluation
235. The Logic of Limax Learning
236. Conditioning and associative learning.
237. Shaping robot behavior using principles from instrumental conditioning
238. Explaining Temporal Differences to Create Useful Concepts for Evaluating States
239. A comparison of natural and artificial intelligence
240. Fast Exact Planning in Markov Decision Processes
241. A Neural Substrate of Prediction and Reward
242. How fast to work - Response vigor, motivation and tonic dopamine
243. Theoretical Analysis of Learning with Reward-Modulated Spike-Timing-Dependent Plasticity
244. A Model of How the Basal Ganglia Generate and Use Neural Signals That Predict Reinforcement
245. Adaptive behavior and learning
246. Improved Allocation of Weights for Associative Memory Storage in Learning Control Systems
247. A Unified Theory of Heuristic Evaluation Functions and its Application to Learning
248. Building and Understanding Adaptive Systems - A Statistical/Numerical Approach to Factory Automation and Brain Research
249. Genetic programming - on the programming of computers by means of natural selection
250. Decisions, Uncertainty, and the Brain - The Science of Neuroeconomics
251. Genetic Algorithms in Search Optimization and Machine Learning
252. Theory and development of higher-order CMAC neural networks
253. Regularization Algorithms for Learning That Are Equivalent to Multilayer Networks
254. Natural Gradient Works Efficiently in Learning
255. Combining online and offline knowledge in UCT
256. Kernel Least-Squares Temporal Difference Learning
257. Quantitative Results Concerning the Utility of Explanation-based Learning
258. How the Basal Ganglia Use Parallel Excitatory and Inhibitory Learning Pathways to Selectively Respond to Unexpected Rewarding Cues
259. Deconstructing the law of effect
260. A Survey of Some Results in Stochastic Adaptive Control
261. Designing Economic Agents that Act Like Human Agents - A Behavioral Approach to Bounded Rationality
262. The convergence of TD(位) for general 位
263. The Roots of Backpropagation - From Ordered Derivatives to Neural Networks and Political Forecasting
264. Determining the Neural Substrates of Goal-Directed Learning in the Human Brain
265. Efficient Learning and Planning Within the Dyna Framework
266. The CDP - A unifying formulation for heuristic search, dynamic programming, and branch-and-bound
267. A Theory of Cerebellar Function
268. Locally Weighted Learning
269. A logical calculus of the ideas immanent in nervous activity
270. Activity in human ventral striatum locked to errors of reward prediction
271. Hierarchical Solution of Markov Decision Processes using Macro-actions
272. Temporal Difference Models and Reward-Related Learning in the Human Brain
273. What is the role of dopamine in reward - hedonic impact, reward learning, or incentive salience?
274. Feature-based methods for large scale dynamic programming
275. Dopamine-mediated regulation of corticostriatal synaptic plasticity
276. Landmark learning - An illustration of associative search
277. Monte Carlo Strategies in Scientific Computing
278. Simulation-based optimization of Markov reward processes
279. Perturbation realization, potentials, and sensitivity analysis of Markov processes
280. Fast Online Policy Gradient Learning with SMD Gain Vector Adaptation
281. Brain Function and Adaptive Systems - A Heterostatic Theory
282. Computational Capabilities of Single Neurons - Relationship to Simple Forms of Associative and Nonassociative Learning in Aplysia
283. Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search
284. Simulation of anticipatory responses in classical conditioning by a neuron-like adaptive element
285. Linear Least-Squares algorithms for temporal difference learning
286. An improved multi-dimensional CMAC neural network - Receptive field function and placement
287. Animal intelligence - An experimental study of the associative processes in animals.
288. An optimal one-way multigrid algorithm for discrete-time stochastic control
289. Simulation of the classically conditioned nictitating membrane response by a neuron-like adaptive element - Response topography, neuronal firing, and interstimulus intervals
290. Toward learning time-varying functions with high input dimensionality
291. Distributed Representations
292. Polynomial approximation-a new computational technique in dynamic programming - Allocation processes
293. Without Miracles - Universal Selection Theory and the Second Darwinian Revolution
294. Extensions of a Theory of Networks for Approximation and Learning
295. A framework for mesencephalic dopamine systems based on predictive Hebbian learning
296. Optimal path-finding algorithms*
297. Responses of monkey dopamine neurons during learning of behavioral reactions.
298. A colony architecture for an artificial creature
299. Behavioral Game Theory - Experiments in Strategic Interaction
300. The neural basis of associative reward learning in honeybees
301. Cognitive maps in rats and men.
302. A theory of causal learning in children - causal maps and Bayes nets.
303. Bandit Based Monte-Carlo Planning
304. A Comparison and Evaluation of Three Machine Learning Procedures as Applied to the Game of Checkers
305. [Random Forests](2004-random-forests)
306. Addiction as a Computational Process Gone Awry
307. Temporal contiguity requirements for long-term associative potentiation/depression in the hippocampus
308. Observable Operator Models for Discrete Stochastic Time Series
309. Programming a computer for playing chess
310. Classical conditioning and brain systems - the role of awareness.
311. A PROBLEM IN THE SEQUENTIAL DESIGN OF EXPERIMENTS
312. Diversity-Based Inference of Finite Automata (Extended Abstract)
313. On the Convergence of Optimistic Policy Iteration
314. Parallel and Distributed Computation - Numerical Methods
315. General Game Playing
316. A Novel Reinforcement Model of Birdsong Vocalization Learning
317. Spike-Timing-Dependent Hebbian Plasticity as Temporal Difference Learning
318. Adaptive linear quadratic control using policy iteration
319. [Pattern Recognition and Machine Learning](2007-pattern-recognition-and-machine-learning)
320. Dopamine neurons encode the better option in rats deciding between differently delayed or sized rewards
321. An N-player sequential stochastic game with identical payoffs
322. A Computational Model of Birdsong Learning by Auditory Experience and Auditory Feedback
323. Games of Stochastic Automata
324. Learning automata - an introduction
325. A Stochastic Learning Model of Economic Behavior
326. Adaptive Coding of Reward Value by Dopamine Neurons
327. Prospect Theory - An Analysis of Decision under Risk
328. Neural Ensembles in CA3 Transiently Encode Paths Forward of the Animal at a Decision Point
329. A chess-playing machine.
330. Functional Imaging of Neural Responses to Expectancy and Experience of Monetary Gains and Losses
331. Learning to Drive a Bicycle Using Reinforcement Learning and Shaping
332. Training Stochastic Model Recognition Algorithms as Networks can Lead to Maximum Mutual Information Estimation of Parameters
333. Average cost temporal-difference learning
334. High-Performance Job-Shop Scheduling With A Time-Delay TD(位) Network
335. Dopamine neurons of the monkey midbrain - contingencies of responses to active touch during self-initiated arm movements.
336. Dissociable Roles of Ventral and Dorsal Striatum in Instrumental Conditioning
337. Approximation by superpositions of a sigmoidal function
338. Empowerment - a universal agent-centric measure of control
339. Generalized polynomial approximations in Markovian decision processes
340. Neurons in the orbitofrontal cortex encode economic value
341. Erratum to - Formation of attentional-associative networks in real time - Role of the hippocampus and implications for conditioning
342. Instrumental Responding following Reinforcer Devaluation
343. Behavior analysis and revaluation.
344. Sample mean based index policies by O(log n) regret for the multi-armed bandit problem
345. Counterfactual Probabilities - Computational Methods, Bounds and Applications
346. Some aspects of the sequential design of experiments
347. Synaptic tagging and long-term potentiation
348. Stable Fitted Reinforcement Learning
349. Sparse distributed memory and related models
350. Applications of advances in nonlinear sensitivity analysis
351. Local Learning Algorithms
352. The nature of explanation
353. Asynchronous Stochastic Approximation and Q-Learning
354. A day of great illumination - B. F. Skinner's discovery of shaping.
355. Dopamine-dependent plasticity of corticostriatal synapses
356. Swinging up the Acrobot - an example of intelligent control
357. Dopamine neurons of the monkey midbrain - contingencies of responses to stimuli eliciting immediate behavioral reactions.
358. The misbehavior of organisms.
359. Discrete-time, discrete-valued observable operator models - a tutorial
360. On the Theory of Apportionment
361. [Gradient-based learning applied to document recognition](1998-gradient-based-learning-applied-to-document-recognition)
362. A neuro-dynamic programming approach to retailer inventory management
363. FUNCTIONAL APPROXIMATIONS AND DYNAMIC PROGRAMMING
364. Predictability Modulates Human Brain Response to Reward
365. Efficient Locally Weighted Polynomial Regression Predictions
366. The problem of expensive chunks and its solution by restricting expressiveness
367. Radial Basis Functions, Multi-Variable Functional Interpolation and Adaptive Networks
368. Cortical Synapses and Reinforcement - a Hypothesis
369. Rollout Algorithms for Combinatorial Optimization
370. Experimental Psychology
371. Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems
372. Deconstructing episodic memory with construction
373. Is there a cell-biological alphabet for simple forms of learning?
374. Expected-Outcome - A General Model of Static Evaluation
375. Distributed Dynamic Programming
376. A normative perspective on motivation
377. Dynamic Programming - Deterministic and Stochastic Models
378. Artificial intelligence - a modern approach, 2nd Edition
379. Learning to control a dynamic physical system
380. The dynamic structure of everyday life
381. Neural networks for control and system identification
382. Exposition of a New Theory on the Measurement of Risk
383. Generalization of pattern recognition in a self-organizing system
384. Associative memory. A system-theoretical approach
385. A neural model of attention, reinforcement and discrimination learning.
386. The role of the basal ganglia in habit formation
387. Computing Machinery and Intelligence
388. Some Studies in Machine Learning Using the Game of Checkers
389. Scheduling and rescheduling with iterative repair
390. Swing up control of the Acrobot
391. Beat the Dealer - A Winning Strategy for the Game of Twenty-One
392. Adaptive filtering prediction and control
393. Second-order conditioning of the rabbit's nictitating membrane response
394. Multidimensional binary search trees used for associative searching
395. Stochastic Systems - Estimation, Identification, and Adaptive Control
396. Regulation of Synaptic Efficacy by Coincidence of Postsynaptic APs and EPSPs
397. Minimum-time control of the Acrobot
398. Acceleration of stochastic approximation by averaging
399. Matters temporal
400. A two-dimensional interpolation function for irregularly-spaced data
401. Real Applications of Markov Decision Processes
402. Temporal primacy overrides prior training in serial compound conditioning of the rabbit's nictitating membrane response
403. Distributed asynchronous computation of fixed points
404. Some Themes and Primitives in Ill-Defined Systems
405. Simulation of self-organizing systems by digital computer
406. Selection by consequences].
407. Recursive estimation and time-series analysis
408. Intrinsic and Extrinsic Motivations - Classic Definitions and New Directions.
409. Basal ganglia
410. Temporal-difference methods and Markov models
411. Fast Curvature Matrix-Vector Products for Second-Order Gradient Descent
412. Further Real Applications of Markov Decision Processes
413. Simulation and the Monte Carlo method
414. Heuristics for Signature Table Analysis as a Pattern Recognition Technique
415. Causal diagrams for empirical research
416. Pattern Recognition and Neural Networks
417. A Survey of Applications of Markov Decision Processes
418. Pharmacokinetics of a novel formulation of ivermectin after administration to goats
419. Why the Law of Effect will not Go Away
420. What are plans for?
421. Stochastic Approximation
422. Mathematical Games
423. Dynamic channel assignment in cellular radio
424. Heuristics - intelligent search strategies for computer problem solving
425. Signature Table Systems and Learning
426. Chemotaxis in Bacteria
427. IS THERE MORE TO UNCERTAINTY THAN SOME PROBABILITY THEORISTS MIGHT HAVE US BELIEVE
428. Memory Controller Optimizations for Web Servers
429. Individual Choice Behavior
430. Toward memory-based reasoning
431. Theories of learning
432. Intelligent Machinery, A Heretical Theory*
433. A time-delay neural network architecture for isolated word recognition
434. Animal Intelligence
435. Comparisons of channel assignment strategies in cellular mobile telephone systems
436. Computer Go
437. Pattern classification and scene analysis
438. Machines who think.
439. An Algorithm for Finding Best Matches in Logarithmic Expected Time
440. Attention-like processes in classical conditioning
441. Splines and efficiency in dynamic programming
442. The Behavior System
443. Hierarchical Learning in Stochastic Domains - Preliminary Results
444. Meaning and Purpose in the Intact Brain - A Philosophical, Psychological, and Biological Account of Conscious Processes
445. Conditioned Reflexes
446. Temporal credit assignment in reinforcement learning
447. Expectation learning in the brain using diffuse ascending projections
448. Learning from delayed rewards
449. Imitation of Life
450. God and Golem, inc. - a comment on certain points where cybernetics impinges on religion
451. Higher Order Conditioning with Constant Motivation
452. Time-Derivative Models of Pavlovian Reinforcement
453. Purposive behavior in animals and men
454. Behavior, the control of perception
455. The goal-gradient hypothesis and maze learning.
456. The role of secondary reinforcement in delayed reward learning.
457. Principles of Behavior
458. The effect of the introduction of reward upon the maze performance of rats
459. Hilgard and Marquis' Conditioning and learning
460. Reward-related signals carried by dopamine neurons.
461. Predictability, surprise, attention, and conditioning
462. Cellular models of reinforcement.
463. State of the Art-A Survey of Partially Observable Markov Decision Processes - Theory, Models, and Algorithms
464. Brownian Motion and Potential Theory
465. CONTRACTION MAPPINGS IN THE THEORY UNDERLYING DYNAMIC PROGRAMMING
466. ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES
467. A Machine that Learns
468. Stochastic Models for Learning
469. Adaptive treatment allocation and the multi-armed bandit problem
470. Radial basis functions for multivariable interpolation - a review
471. Applications of artificial intelligence techniques to a spacecraft control problem
472. Efficient Estimations from a Slowly Convergent Robbins-Monro Process
473. Bacterial chemotaxis as a model behavioral system
474. The organization of behavior - A neuropsychological theory
475. From Chemotaxis to cooperativity - abstract exercises in neuronal learning strategies
476. Une procedure d'apprentissage pour reseau a seuil asymmetrique (A learning scheme for asymmetric threshold networks)
477. A Summary Comparison of CMAC Neural Network and Traditional Adaptive Control Systems
478. A New Machine-Learning Technique Applied to the Game of Checkers
479. The Hedonistic Neuron - A Theory of Memory, Learning and Intelligence
480. Dynamic Programming and Markov Processes
481. The art and theory of dynamic programming
482. Design Improvements in Associative Memories for Cerebellar Model Articulation Controllers (CMAC)
483. Reinforcement learning with selective perception and hidden state
484. Adaptive switching circuits
485. Automaton theory and modeling of biological systems
486. Reinforcement learning with hidden states
487. Optimal control of systems
488. Foundations of conditioning and learning
489. State Functions and Linear Control Systems
490. Contemporary Animal Learning Theory
491. A bioreactor benchmark for adaptive network-based process control
492. Estimator Algorithms for Learning Automata
493. Thinking with the teachable machine
494. Recursive Estimation and Time Series Analysis
495. The Behavior of Organisms
496. The Role of the Critic in Learning Systems
497. Positive reinforcement produced by electrical stimulation of septal area and other regions of rat brain.
498. On thought - the extrinsic theory.
499. A new type of behaviour theory.
500. Secondary reinforcement in rats as a function of information value and reliability of the stimulus.
501. A critical review of latent learning and related experiments.
502. Alopex - a stochastic method for determining visual receptive fields.
503. Classical mechanics
504. Machine Intelligence
505. Learning a Cost-Sensitive Internal Representation for Reinforcement Learning
506. Neural dynamics of adaptive timing and temporal discrimination during associative learning
507. Efficient Algorithms with Neural Network Behavior
508. Multivariable Functional Interpolation and Adaptive Networks
509. Theory and Practice of Recursive Identification
510. Matrix Iterative Analysis
511. The short-latency dopamine signal - a role in discovering novel actions?
512. Learning and memory in the honeybee.
513. Asymptotically Efficient Adaptive Allocation Rules
514. Brains, behavior, and robotics
515. Adaptive Signal Processing
516. Dopamine, learning and motivation
517. Learning internal representations by error propagation
518. Robot Shaping - Developing Autonomous Agents Through Learning
519. Artificial Intelligence through Simulated Evolution
520. On the Existence of Fixed Points for Q-Learning and Sarsa in Partially Observable Domains
521. Models of Learning
522. A Convergent Form of Approximate Policy Iteration
523. Least Squares Policy Evaluation Algorithms with Linear Function Approximation
524. UNH_CMAC Version 2.1 The University of New Hampshire Implementation of the Cerebellar Model Arithmetic Computer - CMAC
525. Batch Value Function Approximation via Support Vectors
526. Learning in Spiking Neural Networks by Reinforcement of Stochastic Synaptic Transmission
527. Approximating Optimal Policies for Partially Observable Stochastic Domains
528. On the Complexity of Solving Markov Decision Problems
